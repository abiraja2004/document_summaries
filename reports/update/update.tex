\documentclass[10pt]{article}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{enumerate,fullpage,amsmath,amssymb}
\usepackage{hyperref}

\hypersetup{%
  colorlinks=true,% hyperlinks will be coloured
}

\title{CS 281 Final Project Proposal: Crime Predictions}
\author{\href{mailto:keskici@college.harvard.edu}{Kevin Eskici} and \href{mailto:luisperez@college.harvard.edu}{Luis A. Perez}, Harvard University}
\date{\today}

\begin{document}

 \begin{center}
  \framebox{
    \vbox{
    \hbox to 6.5in { {CS 182: Artificial Intelligence}
    \hfill Final Project Proposal}
    \vspace{4mm}
    \hbox to 6.5in { {\large \hfill Document Summarization Update \hfill} }
    \vspace{2mm}
    \hbox to 6.5in { {\it Kevin Eskici (keskici@college.harvard.edu) \hfill Luis Antonio Perez (luisperez@college.harvard.edu)} }
    }
  }
  \end{center}

\section{Problem Statement}
The problem has been redefined slightly, as we're now taking an extractive approach to textual summarization. We will be using the NLTK Python Library (\href{http://www.nltk.org/api/nltk.html}{NLTK API}) along with the accompanying texts (\href{http://www.nltk.org/nltk_data/}{Link to Corpus Data}). The reason for this decision is that the library is readily available and provides us with many natural language processing features that would otherwise require a significant amount of time to implement. \\

To see updates on our code, please check our repository. The update is certainly not the latest version of what we have!\\
\href{https://github.com/kandluis/document_summaries}{Github}.

\section{Baseline}
The first step in the project, on which we're still working on but are hopeful to have done
Therefore, given this data, the project goals are as follows. First, define the function $|\cdot|_s : \mathbb{D} \to \mathbb{N}^+$ as the length of the document $D$ in terms of number of sentences (similarly $|\cdot|_w$ for the number of words), where $\mathbb{D}$ is the space of documents. Let $\mathbb{S}$ be the state of possible sentences, and $\mathbb{W}$ the state of our vocabulary words.
\begin{itemize}
\item An extractive summary, $\mathcal{S}$, for a documented $D$ is an ordered collection of the most ``important'' sentences $\{s_i\}_{i=1}^k$. We have that $k$ is a parameter selected by the user with $1 \leq k \leq |D|$.
\item We note that the importance of a sentence, defined as a function $i_s: \mathbb{S} \to [0,1]$. The importance of a sentence, naively, depends on the importance of the words in the sentence, so we can imagine that $i = f \circ i_w \circ g$ where $i_w: \mathbb{W} \to [0,1]$ is the importance of a single word. Note that all of the importance functions have codomain that is normalized, though they are not strictly probability distributions.
\item As per discussion in the literature \cite{sentence_summary} \cite{hmm_summary} \cite{survey}, the importance of a sensence also dependes on the location of the sentence relative to the body of work. Very naively, we modify $i_s' = \mathbb{S} \times \mathbb{N} \to [0,1]$ as the extension of $i_s$ to the space containing indexed locations.
\item As for the importance of a word, we simply measure the number of times the word occurs in our body of work.
$$
i_w(w) = \frac{\text{count}_D(w)}{|D|_w}
$$
\item Putting the above ideas together, we define the importance of a sentence as
$$
i'_s(s,i) = \sum_{w \in s} i_w(w)
$$
where we normalize the results after calculating them by dividing by: $\sum_{(s',i') \in D - \{w\}} i'_s(s',i')$.
\item The last idea involves a further modification to the calculation of the sentence score. Suppose we have selected the set of sentences $\{s_i\}_{i=1}^m$ for our summary, and are looking to select a further sentence $s_{m+1}$. In order to calculate the score of the sentences, we consider the new word space $\mathbb{W'} = \mathbb{W} \setminus A$ where $A = \bigcup_{i=1}^k A_i$ with $A_i = \{w \mid w \in s_i\}$. The idea here being that we do not want to double count words that have already been selected!
\item The final item is that we break ties at random.
\end{itemize}

\section{Current Work}
\subsection{Challenges}
\label{sec:challenges}
We've struggled with finding a good source of training data for our model, as has been highlighted above. In the weeks up to this update, we've spent most of our time researching current models. We've actually diverged from our original plan of utilizing an HMM due to the lack-luster results we've found in other papers (after further research). Additionally, the lack of good training data (it's surprising that papers don't provide their source code/data along with the paper itself) has led us to instead attempt and unsupervised approach to the problem.

\subsection{Implementation of Baselines}
\label{sec:baseline}
The baselines are not yet implemented, as of the writing of this update. Please check our github for further updates.
% TODO(keskici)

\subsection{Implementation of Grasshopper Algorithm}
\label{sec:grasshopper}
The Grasshopper algorithm is described in the \cite{grasshopper} has been implemented and resides under the grasshopper directory in our code repository. We now present a brief overview of this algorithm so the discussion in the Section \ref{sec:future_work} section makes sense.
\begin{itemize}
\item Grasshopper creates a Markov model of the given document. Each sentence is represented as a node in the graph, with edges to all other sentence with weight $w_ij$ which is dependent on the cosine similarity between sentence $i$ and sentence $j$. We use the TF-IDF vectors for each sentence. Note that a sparce graph is enforced by using a $0.1$ thresh-hold.
\item We further modify $w_ij$ by taking into account the prior $r$, over the sentence importance.
\item Grasshopper then finds the steady state of this matrix model.
\item From the steady state, the state with the largest probability distribution ($s_{max}$) is selected.
\item The graph is then modified to change the state $s_{max}$ into an absorption state.
\item Repeat the the above steps (excluding the first) until a total of $k$ sentences are extracted, as specified by the user.
\end{itemize}
The algorithm is relatively simple. Results are presented below for some sample test.
% TODO(luisperez)

\subsection{Implementation of TextRank}
\label{sec:textrank}
[u'monthly payroll',
 u'union payroll',
 u'individual union',
 u'payroll reduction',
 u'county home',
 u'county school',
 u'brick home',
 u'bedroom floor']

For citation, just use \cite{textrank}.
% TODO(keskici)

\subsection{Future Work}
\label{sec:future_work}
The first step is to implement tests. While some of our current results appear promising, we plan on using ROUGE \cite{rouge} along with a yet to be discovered corpus to help us compare our summaries to those of humans (or generated by other machines?).

For the Grasshopper algorithm, once we have found some training data, we plan to utilize the training set to optimize our $\lambda, \alpha$ parameters to the model. These optimizations can be done through Bayesian Optimization using the \href{https://github.com/HIPS/Spearmint}{Spearmint}. Following the suggestions in \cite{grasshopper}, we also plan to to generalize the grasshopper algorithm to consider ``partial absorbtion'' rather than full absorption, allowing for more variation in our sentence selection. Additionally, when learning the parameters, we plan to utilize different forms of priors $r$ based on textual analysis of the documents. We will compare these results to those that we've achieved thus far and select the most optimal model. Lastly, we might look into changes the $0.1$ threshold used above to enforce sparsity.

% TODO(keskici)

\section{Presentation Type}
We prefer presenting a poster at the poster session on 12/9.

\bibliographystyle{alpha}
\bibliography{../references}

\end{document}
