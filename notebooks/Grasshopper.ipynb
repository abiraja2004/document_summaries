{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Contains useful utility functions use throughout other areas in our code.\n",
    "'''\n",
    "\n",
    "class MyCounter(Counter):\n",
    "    \"\"\"\n",
    "    A myCounter keeps track of counts for a set of keys.\n",
    "\n",
    "    The myCounter class is an extension of the the standard python Counter class.\n",
    "\n",
    "    The myCounter also includes additional functionality useful in implementing\n",
    "    the vectorization of sentences. In particular, counters can be normalized,\n",
    "    multiplied, etc.\n",
    "    \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        self.setdefault(idx, 0)\n",
    "        return dict.__getitem__(self, idx)\n",
    "\n",
    "    def incrementAll(self, keys, count):\n",
    "        \"\"\"\n",
    "        Increments all elements of keys by the same count.\n",
    "\n",
    "        >>> a = Counter()\n",
    "        >>> a.incrementAll(['one','two', 'three'], 1)\n",
    "        >>> a['one']\n",
    "        1\n",
    "        >>> a['two']\n",
    "        1\n",
    "        \"\"\"\n",
    "        for key in keys:\n",
    "            self[key] += count\n",
    "\n",
    "    def argMax(self):\n",
    "        \"\"\"\n",
    "        Returns the key with the highest value.\n",
    "        \"\"\"\n",
    "        if len(self.keys()) == 0: return None\n",
    "        all = self.items()\n",
    "        values = [x[1] for x in all]\n",
    "        maxIndex = values.index(max(values))\n",
    "        return all[maxIndex][0]\n",
    "\n",
    "    def sortedKeys(self):\n",
    "        \"\"\"\n",
    "        Returns a list of keys sorted by their values.  Keys\n",
    "        with the highest values will appear first.\n",
    "\n",
    "        >>> a = Counter()\n",
    "        >>> a['first'] = -2\n",
    "        >>> a['second'] = 4\n",
    "        >>> a['third'] = 1\n",
    "        >>> a.sortedKeys()\n",
    "        ['second', 'third', 'first']\n",
    "        \"\"\"\n",
    "        sortedItems = self.items()\n",
    "        compare = lambda x, y:  sign(y[1] - x[1])\n",
    "        sortedItems.sort(cmp=compare)\n",
    "        return [x[0] for x in sortedItems]\n",
    "\n",
    "    def totalCount(self):\n",
    "        \"\"\"\n",
    "        Returns the sum of counts for all keys.\n",
    "        \"\"\"\n",
    "        return sum(self.values())\n",
    "\n",
    "    def normalize(self):\n",
    "        \"\"\"\n",
    "        Edits the counter such that the total count of all\n",
    "        keys sums to 1.  The ratio of counts for all keys\n",
    "        will remain the same. Note that normalizing an empty\n",
    "        Counter will result in an error.\n",
    "        \"\"\"\n",
    "        total = float(self.totalCount())\n",
    "        if total == 0: return\n",
    "        for key in self.keys():\n",
    "            self[key] = self[key] / total\n",
    "\n",
    "    def divideAll(self, divisor):\n",
    "        \"\"\"\n",
    "        Divides all counts by divisor\n",
    "        \"\"\"\n",
    "        divisor = float(divisor)\n",
    "        for key in self:\n",
    "            self[key] /= divisor\n",
    "\n",
    "    def copy(self):\n",
    "        \"\"\"\n",
    "        Returns a copy of the counter\n",
    "        \"\"\"\n",
    "        return MyCounter(dict.copy(self))\n",
    "\n",
    "    def __mul__(self, y ):\n",
    "        \"\"\"\n",
    "        Multiplying two counters gives the dot product of their vectors where\n",
    "        each unique label is a vector element.\n",
    "\n",
    "        >>> a = Counter()\n",
    "        >>> b = Counter()\n",
    "        >>> a['first'] = -2\n",
    "        >>> a['second'] = 4\n",
    "        >>> b['first'] = 3\n",
    "        >>> b['second'] = 5\n",
    "        >>> a['third'] = 1.5\n",
    "        >>> a['fourth'] = 2.5\n",
    "        >>> a * b\n",
    "        14\n",
    "        \"\"\"\n",
    "        sum = 0\n",
    "        x = self\n",
    "        if len(x) > len(y):\n",
    "            x,y = y,x\n",
    "        for key in x:\n",
    "            if key not in y:\n",
    "                continue\n",
    "            sum += x[key] * y[key]\n",
    "        return sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stationary(Mat, epsilon=0.0001):\n",
    "    '''\n",
    "    Given numpy matrix Mat, returns the vector s such that sX = s, where s is normalized \n",
    "    to be a probabiliy distribution.\n",
    "    So we have sX = sI -> s(X-I) = 0, so we need to find ker(X-I).\n",
    "    We use the linealg package in numpty to take care of this for us. \n",
    "    '''\n",
    "    values, vectors = np.linalg.eig(Mat.T)\n",
    "    \n",
    "    # Due to floating point imprecision, need to use epsilon values!\n",
    "    index = np.nonzero(abs(values - 1.0) < epsilon)\n",
    "    q = vectors[:,index]\n",
    "    \n",
    "    return q / np.sum(q)  # convert into probability distribution\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def invertMatrixTheorem(A, Ainv, indx):\n",
    "    '''\n",
    "    Computes the inverse of a matrix with one row and column removed using\n",
    "    the matrix inversion lemma. It needs a matrix A, the inverse of A,\n",
    "    and the row and column index which needs to be removed.\n",
    "    '''\n",
    "    n,m = A.shape\n",
    "    assert(n == m)  # square matrix\n",
    "    \n",
    "    # Remove row and compute inverse\n",
    "    u = np.zeros(n)\n",
    "    u[indx] = -1\n",
    "    \n",
    "    v = A[indx, :]\n",
    "    \n",
    "    T1 = v.dot(Ainv)\n",
    "    T1.shape = (1, n)\n",
    "    T2 = Ainv.dot(u.T)\n",
    "    T2.shape = (n, 1)\n",
    "    T = Ainv - T2.dot(T1) / (1 + T1.dot(u.T))\n",
    "    \n",
    "    # Remove column and compute inverse. \n",
    "    w = A[:, indx]\n",
    "    w.shape = n\n",
    "    w[indx] = 0\n",
    "    \n",
    "    R1 = T.dot(w)\n",
    "    R1.shape = (n,1)\n",
    "    R2 = u.T.dot(T)\n",
    "    R2.shape = (1,n)\n",
    "    \n",
    "    R = T - R1.dot(R2) / (1 + R2.dot(w))\n",
    "    \n",
    "    # Remove redundant rows\n",
    "    R = np.delete(R, (indx), axis=0)\n",
    "    R = np.delete(R, (indx), axis=1)\n",
    "    \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Grasshopper: Python implementation of re-ranking algorithm with absobing states.\n",
    "\n",
    "For a Matlab Implementation, see http://pages.cs.wisc.edu/~jerryzhu/pub/grasshopper.m\n",
    "'''\n",
    "\n",
    "def grasshopper(W, r, lamb, k, epsilon=0.0001):\n",
    "    '''\n",
    "    INPUTS:\n",
    "        W: Matrix representataion of an directed, weighed graph. Self-edges are allowed.\n",
    "            Undirected graphs should be symetric.\n",
    "        r: A probability distribution over the sentences, with higher probability implying higher \n",
    "            ranking.\n",
    "        lamb: How much to weight the prior in comparison to the input graph weights.\n",
    "        k: return the top k items as ranked by Grasshopper\n",
    "        \n",
    "    OUTPUT\n",
    "        res: a list of (index, prob) of length k, where index corresponds to the index in the \n",
    "        input graph, and prob is the probability assigned to it when selected.\n",
    "        \n",
    "    The algorithm is modified to take advantage of the fact that we only care about the expected\n",
    "    number of states. \n",
    "    '''\n",
    "    # Let's do some basis error checking!\n",
    "    n,m = W.shape\n",
    "    assert(n == m) # Sizes should be equal\n",
    "    assert(np.min(W) >= 0) # No negative edges\n",
    "    assert(abs(np.sum(r)- 1) < epsilon) # r is a distribution, floating point imprecision\n",
    "    assert(0 <= lamb and lamb <= 1) # lambda is valid\n",
    "    assert(0 < k and k <= n) # Summary can't be longer than document!\n",
    "\n",
    "    # Normalize the rows of W to create the transition matrix P'\n",
    "    P = W / np.sum(W, axis=1)\n",
    "    hatP = lamb * P - (1 - lamb) * r\n",
    "    \n",
    "    assert(hatP.shape == (n,m)) #  Shape should not change!\n",
    "    \n",
    "    # To store results.\n",
    "    absorbed = []\n",
    "    nonAbsorbed = range(n)\n",
    "    probs = []\n",
    "    \n",
    "    # Calculate the most probable state!\n",
    "    q = stationary(hatP);\n",
    "    absorbed.append(np.argmax(q))\n",
    "    probs.append(np.max(q))\n",
    "    nonAbsorbed.remove(np.argmax(q))\n",
    "    \n",
    "    # Compute the inverse of the fundamental matrix!\n",
    "    N = np.linalg.inv(np.identity(len(nonAbsorbed)) - hatP[nonAbsorbed, nonAbsorbed])\n",
    "    \n",
    "    # Pick the ramaining k-1 items by picking out the most-visited node one by one.\n",
    "    # once picked out, the item turns into an absorbing node.\n",
    "    while (len(absorbed)<k):\n",
    "        # Compute expected number fo times each node will be visited before random walk is absorbed by\n",
    "        # absorbing nodes. Averaged over all start nodes.\n",
    "    \n",
    "        # Compute the expected visit counts\n",
    "        nuvisit = np.sum(N, axis=0)\n",
    "        nvisit = np.zeros(n)\n",
    "        nvisit[nonAbsorbed] = nuvisit\n",
    "\n",
    "        # Find the new absorbing state\n",
    "        absorbState = np.argmax(nvisit)\n",
    "        absorbVisit = max(nvisit)\n",
    "        # Store the results\n",
    "        absorbed.append(absorbState)\n",
    "        probs.append(absorbVisit)\n",
    "        \n",
    "        # Compute the inverse using the matrix inversion theorem to avoid re-computation!\n",
    "        index = np.nonzero(abs(nonAbsorbed - absorbState) < epsilon)  # Compute the index of the absorbedState in relation to the submatrix Q (see paper for details)\n",
    "        N = invertMatrixTheorem(np.identity(len(nonAbsorbed)) - hatP[nonAbsorbed, nonAbsorbed], N, index);\n",
    "        \n",
    "        # Update the nonAbsorbed states\n",
    "        nonAbsorbed.remove(absorbState)\n",
    "        \n",
    "    # Return the results!\n",
    "    return zip(absorbed, probs)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean(w):\n",
    "    '''\n",
    "    Given a word, removes all non-alphabetic starting/ending characters. It then proceeds to check \n",
    "    if remaining characters are alphabetic. If so, return those characters. If not, returns None\n",
    "    '''\n",
    "    # Use regex!\n",
    "    return w\n",
    "\n",
    "def cosineSim(v1, v2):\n",
    "    '''\n",
    "    Note that input vectors are sparse!\n",
    "    '''\n",
    "    nv1 = v1.copy()\n",
    "    nv2 = v2.copy()\n",
    "    nv1.normalize()\n",
    "    nv2.normalize()\n",
    "    return nv1 * nv2\n",
    "\n",
    "def thresholdCosineSim(v1,v2, threshold = 0.01):\n",
    "    score = cosineSim(v1,v2)\n",
    "    return 0 if score < threshold else score\n",
    "    \n",
    "def tf_idf(sentence):\n",
    "    '''\n",
    "    Given a sentence, converts the sentence to a TF-IDF representations. The representation is parse,\n",
    "    with the key being the term. \n",
    "    '''\n",
    "    v1 = MyCounter()\n",
    "    # TODO(nautilik): Need to avoid stop words, non-english words, etc.\n",
    "    for word in sentence:\n",
    "        # Remove starting/ending punctuations/spaces\n",
    "        # Make sure whatever is left is an english word\n",
    "        cleanWord = clean(word)\n",
    "        if cleanWord is not None:\n",
    "            v1[cleanWord.lower()] +=1\n",
    "        \n",
    "    return v1\n",
    "\n",
    "def docToMatrix(D, vec_fun=tf_idf, sim_fun=cosineSim):\n",
    "    '''\n",
    "    Given a document d which consists of a set of sentences, converts it into a |D|_s x |D|_s matrix \n",
    "    with weights given by the specifiend similarity function. The similary function should take \n",
    "    as input vector representations as output by the vec_fun. \n",
    "    '''\n",
    "    # Convert sentences to vector representations!\n",
    "    sentenceVectors = [vec_fun(s) for s in D]\n",
    "    \n",
    "    # Compute similaliry\n",
    "    n = len(D)\n",
    "    M = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            M[i,j] = sim_fun(sentenceVectors[i], sentenceVectors[j])\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = docToMatrix(brown.sents('ca19'), sim_fun=thresholdCosineSim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = np.ones(len(W))\n",
    "r = r / np.sum(r)\n",
    "results = grasshopper(W, r, 1.0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(32, 0.0088495575221240533),\n",
       " (70, 0.78209433481689405),\n",
       " (2, 0.76883969341860658),\n",
       " (30, 0.7405060383741493),\n",
       " (10, 0.73698511909308306)]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Howard E. Simpson , the railroad's president , said , `` A drastic decline in freight loading due principally to the severe slump in the movement of heavy goods has necessitated this regrettable action '' .\n",
      "Since the railroad cannot reduce the salary of individual union members under contract , it must accomplish its payroll reduction by placing some of the men on furlough , a B. & O. spokesman said .\n",
      "The proposal was made by Dr. David S. Jenkins after he and Mrs. D. Ellwood Williams , Jr. , a board member and long-time critic of the superintendent , argued for about fifteen minutes at this week's meeting .\n",
      "Cites discrepancies\n",
      "Soon after 10 A.M. , when police reached the 1-1/2-story brick home in the Franklin Manor section , 15 miles south of here on the bay , in response to a call from the Dresbach's other son , Lee , 14 , they found Mrs. Dresbach's body on the first-floor bedroom floor .\n"
     ]
    }
   ],
   "source": [
    "for (i, p) in sorted(results, key=lambda x: x[0]):\n",
    "    print ' '.join(brown.sents('ca19')[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
